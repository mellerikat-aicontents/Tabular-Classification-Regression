name: tcr
version: 2.1.2

## 외부에서 데이터 가져오기 / 결과 저장하는 경우 해당 위치에 지정
external_path:
    - load_train_data_path: ./solution/sample_data/train
    - load_inference_data_path:  ./solution/sample_data/test
    - save_train_artifacts_path:
    - save_inference_artifacts_path:
    - load_model_path:

external_path_permission:
    - aws_key_profile: 
 
## 실험에 필요한 파라미터를 설정함 
## - 해당 위치에서 삭제되면, code 의 default 로 실행
user_parameters:
    - train_pipeline:
        - step: input 
          args:
            - file_type: csv
              encoding: utf-8
          ui_args: 
        
        - step: readiness
          args:
            - x_columns: [input_x0,input_x1,input_x2,input_x3]
              y_column: target
              task_type: classification
              target_label: _major
              column_types: auto
          ui_args:
            - x_columns
            - y_column

        - step: preprocess
          args:
          ui_args:

        - step: sampling
          args:
            - data_split: {method: cross_validation, options: 3}
          ui_args:

        - step: train
          args:
            - evaluation_metric: auto
              shapley_value: False 
              output_type: all
          ui_args:
            - evaluation_metric
            - shapley_value

        - step: output
          args:
          ui_args:

    - inference_pipeline:
        - step: input  
          args:
          ui_args: 
        
        - step: readiness
          args:
          ui_args:

        - step: preprocess
          args:
          ui_args:

        - step: inference
          args:
          ui_args:

        - step: output
          args:
          ui_args:

## asset 의 설치 정보를 기록       
asset_source:
    - train_pipeline:
        - step: input
          source: 
            code: https://github.com/mellerikat-aicontents/input.git
            # code: local
            branch: v1.0.1_tabular    
            requirements:
              - pandas==1.5.3
              - numpy==1.26.4

        - step: readiness
          source:  
            code: https://github.com/mellerikat-aicontents/readiness.git
            # code: local
            branch: v1.0.3_tcr
            requirements:
              - requirements.txt

        - step: preprocess
          source:  
            code: https://github.com/mellerikat-aicontents/preprocess.git
            # code: local
            branch: v2.0.3_tcr
            requirements:
              - requirements.txt

        - step: sampling
          source:  
            code: https://github.com/mellerikat-aicontents/sampling.git
            # code: local
            branch: v1.0.2_tabular
            requirements:
              - requirements.txt

        - step: train
          source:
            code: https://github.com/mellerikat-aicontents/tcr_modeling_ml.git
            # code: local
            branch: v2.1.2
            requirements:
              - requirements.txt 
    
        - step: output
          source:
            code: https://github.com/mellerikat-aicontents/output.git
            # code: local
            branch: v1.0.0
            requirements:
              - requirements.txt
   
    - inference_pipeline:
        - step: input
          source:  
            code: https://github.com/mellerikat-aicontents/input.git
            # code: local
            branch: v1.0.1_tabular   
            requirements:
              - pandas==1.5.3
              - numpy==1.26.4

        - step: readiness
          source:  
            code: https://github.com/mellerikat-aicontents/readiness.git
            # code: local
            branch:  v1.0.3_tcr
            requirements:
              - requirements.txt

        - step: preprocess
          source:   
            code: https://github.com/mellerikat-aicontents/preprocess.git
            # code: local
            branch: v2.0.3_tcr
            requirements:
              - requirements.txt

        - step: inference
          source:
            code: https://github.com/mellerikat-aicontents/tcr_modeling_ml.git
            # code: local
            branch: v2.1.2
            requirements:
              - requirements.txt 
    
        - step: output
          source:
            code: https://github.com/mellerikat-aicontents/output.git
            # code: local
            branch: v1.0.0
            requirements:
              - requirements.txt
           
     
control:
    ## 1. whether to install assets and dependency packages once or every time. 
    - get_asset_source: once ## once, every
    ## 2. whether to backup saved artifacts
    - backup_artifacts: True ## True, False
    ## 3. whether to backup log
    - backup_log: True  ## True, False
    ## 4. determine artifacts backup size(MB)
    - backup_size: 1000
    ## 5. asset data, config interfacing method - memory: (fast) / file: (saved; non-volatilizing) 
    - interface_mode: memory ## memory, file
    ## 6. inference artifacts compression format 
    - save_inference_format: tar.gz ## tar.gz, zip
    ## 7. resource check 
    - check_resource: False ## True: measure memory, cpu / False  


########
ui_args_detail:
    - train_pipeline:
        - step: input 
          args:
              - name: file_type
                description: Enter the file extension of the input data. (Currently, only csv files can be registered for AI Solution.)
                type: single_selection
                default: csv
                selectable:
                  - csv
              - name: encoding
                description: Enter the encoding type of the input data. (Currently, only utf-8 can be registered for AI Solution.)
                type: string
                default: utf-8
                range:
                  - 1
                  - 1000000
        - step: readiness 
          args:
              - name: x_columns
                description: Enter the names of the x columns to be trained in the Dataframe, separated by ','.
                type: string
                default: ''
                range:
                  - 1
                  - 1000000
              - name: y_column
                description: Enter the name of the y column in the Dataframe.
                type: string
                default: ''
                range:
                  - 1
                  - 1000000
              - name: task_type
                description: Enter the type of Solution task (classification/regression).
                type: single_selection
                default: classification
                selectable:
                  - classification
                  - regression
              - name: drop_x_columns
                description: Enter the columns that will not be used for training. The remaining columns, excluding the columns you entered, y_column, and groupkey_columns, will be used for training. If you enter an empty list [], all columns of the input data will be used for training. If there are multiple columns, enter them separated by ','.
                type: string
                default: ''
                range:
                  - 1
                  - 1000000
              - name: groupkey_columns
                description: If you enter a groupkey column, the dataframe is grouped based on the value of that column. If there are multiple columns, enter them separated by ','.
                type: string
                default: ''
                range:
                  - 1
                  - 1000000
        - step: preprocess
          args:
              - name: save_original_columns
                description: This determines whether to replace the original training columns (x_columns) with the preprocessed training columns. If True, both the original x_columns and the preprocessed x_columns are passed to the next asset together. If False, the original x_columns are replaced with the preprocessed x_columns and are not passed to the next asset.
                type: single_selection
                default: True
                selectable:
                  - True
                  - False    
        - step: train 
          args:
              - name: evaluation_metric
                description: When performing Hyperparameter Optimization (HPO), you select an evaluation metric to choose the model. This metric will be used to compare the performance of different models and select the best one.
                type: single_selection
                default: auto
                selectable:
                  - auto
                  - accuracy
                  - f1
                  - recall
                  - precision
                  - mse
                  - r2
                  - mae
                  - rmse    
              - name: shapley_value
                description: It decides whether to calculate the Shapley value, a measure of feature importance in machine learning, and output it to output.csv.
                type: single_selection
                default: False
                selectable:
                  - False
                  - True
              - name: output_type
                description: This determines whether to save all columns and modeling result columns to output.csv (all), or to save only the modeling columns (simple).
                type: single_selection
                default: all
                selectable:
                  - all
                  - simple
              - name: multiprocessing
                description: You can set the use of multiprocessing to either True or False.
                type: single_selection
                default: True
                selectable:
                  - True
                  - False
              - name: num_cpu_core
                description:  Enter the number of CPU cores to use for multiprocessing.
                type: int
                default: 
                  - 3
                range:
                  - 1
                  - 100000
    - inference_pipeline: