## 외부에서 데이터 가져오기 / 결과 저장하는 경우 해당 위치에 지정
external_path:
    - load_train_data_path: /nas001/users/yoonji.suh/tcr_test_20231011/train_missing/
    - load_inference_data_path: /nas001/users/yoonji.suh/tcr_test_20231011/inf/
    - save_train_artifacts_path:
    - save_inference_artifacts_path:

external_path_permission:
    - s3_private_key_file: /nas001/users/woosung.jang/aws_key
 
## 실험에 필요한 파라미터를 설정함 
## - 해당 위치에서 삭제되면, code 의 default 로 실행
user_parameters:
    - train_pipeline:
        - step: input  ## 필수
          args:
            - input_path: train_missing
              x_columns: [Count,Converted Torque,angle_min,angle_mean,angle_median,angle_std,angle_max,torque_min,torque_mean,torque_median,torque_std,torque_max]
              use_all_x: False
              y_column: label #TRAIN_LABEL" #label"
              groupkey_columns:
              drop_columns:
              time_column:

        - step: preprocess
          args:
            - handling_missing: interpolation # dropna, interpolation, fill_number
              handling_outlier_x: none        #none, distribution, spec_limit, specout_drop, exponential
              handling_scaling_x: none  #none"         #none, standard, minmax, abs, robust
              handling_encoding_x_columns: #Budget Actual,Business Unit" # categorical columns 
              handling_encoding_y_column: label #TRAIN_LABEL" # y (=label)이 categorical인 경우 설정
              handling_encoding_x: none #onehot"    ## none, label, ordinal, binary, onehot, hashing
              handling_encoding_y: label    ## none, label, ordinal, binary, onehot, hashing
              limit_encoding_categories: 30 # onehot이나 hashing 인코딩 등을 진행 시 컬럼이 너무 많아지는 것에 대한 한계치 설정 
              handling_imputer_x: none #mean"  ## none, mean, median, most_frequent
              handling_imputer_y: none  ## none, mean, median, most_frequent
              drop_duplicate_time: True   ## group 단위로 중복 확인 (time_column 없으면 미진행)
              load_train_preprocess: False   ## (inference workflow 전용) True 이면, train preprocess 를 참조하여 진행
              handling_downsampling_interval: 0   ## 0, 1, 2,... (0 이면 사용하지 않음)
              downsampling_method: median #first"   ## first, last, mean, median 
        
        - step: sampling
          args:
            - sampling_type: none # none(실행안함) ,under(under sampling), over(over_sampilng - TBD)
              sampling_method: negative # random(random sampling), cluster(density based clustering sampling), negative(negative sampling)
              label_sampling: True # label 별 sampling 여부 선택 True, False
              ignore_label_class: 1 # 특정 클래스에 있는 데이터 전부 사용. cat or cat,dog
              negative_target_class: # negative sampling 시 target이 되는 class
              label_sampling_num_type: compare # ratio, number, compare, mingroup
              label_sampling_num: {1: 1, 0: 25}
              sampling_groupkey_columns: # groupkey 컬럼 입력. 3개 까지 지원
              sampling_num_type: # ratio, number, mingroup
              sampling_num: 
        
        - step: train ## 필수
          args:
            - model_type: classification ## supproted list: classification, regression
              data_split_method: cross_validate # supproted list: cross_validate, train_test_split
              evaluation_metric: accuracy ## classification: accuracy, precision, recall, f1-score
              model_list: [lgb, rf, gbm, cb] # 알고리즘 선택(param_range를 사용할 경우, classification의 경우 rf, gbm, lgb, cb만 가능)
              num_hpo: 3 # HPO 개수 설정, 0 인 경우 하기 설정을 무시하고 위에서 선택한 알고리즘에 대해서 default로 실행됨
              param_range: {
                rf: {max_depth: 6, n_estimators: [300, 500]},
                gbm: {max_depth: [5, 7], n_estimators: [300, 500]},
                ngb: {col_sample: [0.6, 0.8], n_estimators:[100, 300]},               
                lgb: {max_depth:[5, 9], n_estimators:[300, 500]},
                cb: {max_depth:[5, 9], n_estimators:[100, 500]},
              } # 탐색하고 싶은 parameter [min, max] 설정, 숫자만 입력할 경우 해당 parameter는 고정됨
              ## num_hpo=3, max_depth=[1, 3]인 경우 hpo 과정에서 max_depth가 1, 2, 3인 경우에 대해 실행됨.(소수점인 경우 integer로 실행)
              shap_ratio: 1.
              evaluation_report: False   ## True, False (speed-up) 
   
    - inference_pipeline:
        - step: input  ## 필수
          args:
            - input_path: inf
              x_columns: [Count,Converted Torque,angle_min,angle_mean,angle_median,angle_std,angle_max,torque_min,torque_mean,torque_median,torque_std,torque_max]
              use_all_x: False
              y_column:  #TRAIN_LABEL" #label"
              groupkey_columns:
              drop_columns:
              time_column:
              
        - step: preprocess
          args:
            - handling_missing: interpolation
              handling_outlier_x: none        #none, distribution, spec_limit, specout_drop, exponential
              handling_scaling_x: none  #none"         #none, standard, minmax, abs, robust
              handling_encoding_x_columns: #Budget Actual,Business Unit" # categorical columns 
              handling_encoding_y_column:  #TRAIN_LABEL" # y (=label)이 categorical인 경우 설정
              handling_encoding_x: none #onehot"    ## none, label, ordinal, binary, onehot, hashing
              handling_encoding_y: none    ## none, label, ordinal, binary, onehot, hashing
              limit_encoding_categories: 30 # onehot이나 hashing 인코딩 등을 진행 시 컬럼이 너무 많아지는 것에 대한 한계치 설정 
              handling_imputer_x: none #mean"  ## none, mean, median, most_frequent
              handling_imputer_y: none  ## none, mean, median, most_frequent
              drop_duplicate_time: True   ## group 단위로 중복 확인 (time_column 없으면 미진행)
              load_train_preprocess: True   ## (inference workflow 전용) True 이면, train preprocess 를 참조하여 진행
              handling_downsampling_interval: 0   ## 0, 1, 2,... (0 이면 사용하지 않음)
              downsampling_method: median #first"   ## first, last, mean, median 

        - step: inference ## 필수
          args:
            - model_type: classification
   
## asset 의 설치 정보를 기록       
asset_source:
    - train_pipeline:
        - step: input
          source:  ## git / local 지원
            code: http://mod.lge.com/hub/smartdata/ml-framework/alov2-module/input.git
            # code: local
            branch: tabular
            requirements:
              - pandas==1.5.3

        - step: preprocess
          source:  ## git / local 지원
            code: http://mod.lge.com/hub/smartdata/ml-framework/alov2-module/preprocess.git
            # code: local
            branch: tcr 
            requirements:
              - pandas==1.5.3
              - category_encoders

        - step: sampling
          source:
            code: http://mod.lge.com/hub/smartdata/aiplatform/module/data_sampling.git
            # code: local
            branch: main
            requirements:
              - requirements.txt 

        - step: train
          source:
            code: http://mod.lge.com/hub/smartdata/ml-framework/alov2-module/train.git
            # code: local
            branch: tcr
            requirements:
              - requirements.txt 
   
    - inference_pipeline:
        - step: input
          source:  ## git / local 지원
            code: http://mod.lge.com/hub/smartdata/ml-framework/alov2-module/input.git
            # code: local
            branch: tabular
            requirements:
              - pandas==1.5.3

        - step: preprocess
          source:  ## git / local 지원
            # code: http://mod.lge.com/hub/smartdata/ml-framework/alov2-module/preprocess.git
            code: local
            branch: tcr
            requirements:
              - pandas==1.5.3

        - step: inference
          source:
            code: http://mod.lge.com/hub/smartdata/ml-framework/alov2-module/inference.git
            # code: local
            branch: tcr
            requirements:
              - pandas==1.5.3
           
     
control:
    ## 1. 패키지 설치 및 asset 존재 여부를 실험 시마다 체크할지, 한번만 할지 결정
    ## 1-2 requirements.txt 및 종속 패키지들 한번만 설치할 지 매번 설치할지도 결정 
    - get_asset_source: once ## once, every
    # TODO 아래 get_external_data 제작하기
    - get_external_data: once ## once, every
    ## 2. 생성된 artifacts 를 backup 할지를 결정 True/False
    - backup_artifacts: True
    ## 3. pipeline 로그를 backup 할지를 결정 True/False
    - backup_log: True
    ## 4. 저장 공간 사이즈를 결정 (단위 MB)
    - backup_size: 1000
 
    ## 5. Asset 사이 데이터 전달 방법으로 memory, file 를 지원
    - interface_mode: memory
